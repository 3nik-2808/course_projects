# -*- coding: utf-8 -*-
"""AI_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WIQkMJENrEKHNJjtGudhoyh4-O4QQmjT
"""

# Commented out IPython magic to ensure Python compatibility.
import string
import re
from numpy import array, argmax, random, take
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, RepeatVector, Bidirectional, Attention, Input, Dropout
from keras.preprocessing.text import Tokenizer, tokenizer_from_json
from keras.callbacks import ModelCheckpoint
from keras.utils import pad_sequences
from keras.models import load_model
from keras import optimizers
import matplotlib.pyplot as plt
import io
import json

# %matplotlib inline
pd.set_option('display.max_colwidth', 200)

# Mount Google Drive to Colab
from google.colab import drive
drive.mount('/content/drive')

with open('/content/drive/MyDrive/Project/AI/tokenizer/eng_tokenizer.json') as f:
    eng_data = json.load(f)
    eng_tokenizer = tokenizer_from_json(eng_data)

eng_vocab_size = len(eng_tokenizer.word_index) + 1

eng_length = 40
print('English Vocabulary Size: %d' % eng_vocab_size)

with open('/content/drive/MyDrive/Project/AI/tokenizer/vi_tokenizer.json') as f:
    vi_data = json.load(f)
    vi_tokenizer = tokenizer_from_json(vi_data)

vi_vocab_size = len(vi_tokenizer.word_index) + 1

vi_length = 60
print('Vietnamese Vocabulary Size: %d' % vi_vocab_size)

model = load_model('/content/drive/MyDrive/Project/AI/model/lstm_en_vi_1.1.h5')

def encode_sequences(tokenizer, length, lines):
    seq = tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    seq = pad_sequences(seq, maxlen=length, padding='post')
    return seq

def get_word(n, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == n:
            return word
    return None

en_test = ["I am a human.", "She has two children", "Who are you?", "He has water.", "She is beautiful.", "Where are you?",
           "This is a beautiful city.", "That family has four people", "I am working."]
vi_test = ["Tôi là một con người", "Cô ấy có hai đứa trẻ", "Bạn là ai?", "Anh ấy có nước.", "Cô ấy đẹp.", "Bạn đang ở đâu?",
           "Đây là một thành phố đẹp.", "Gia đình đó có bốn người.", "Tôi đang làm việc"]

testX = encode_sequences(eng_tokenizer, eng_length, en_test)
testY = encode_sequences(vi_tokenizer, vi_length, vi_test)

preds = argmax(model.predict(testX.reshape((testX.shape[0],testX.shape[1]))), axis=-1)

preds_text = []
for i in preds:
    temp = []
    for j in range(len(i)):
        t = get_word(i[j], vi_tokenizer)
        if j > 0:
            if (t == get_word(i[j-1], vi_tokenizer)) or (t == None):
                temp.append('')
            else:
                temp.append(t)
        else:
            if(t == None):
                temp.append('')
            else:
                temp.append(t) 

    preds_text.append(' '.join(temp))

pred_df = pd.DataFrame({'original': en_test,'actual' : vi_test, 'predicted' : preds_text})

pred_df.head(10)